{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Semantic & Influence Networks\n",
    "\n",
    "This week, we explore the representation and analysis of semantic networks. A word or document network is an unsupervized representation of text akin to a clustering or an embedding, but semantic networks can also be defined using semantic or syntactic information derived from methods we have used earlier in the quarter. For example, we can define links between words as a function of their co-presence within a document, chapter, paragraph, sentence, noun phrase or continuous bag of words. We can also define links as a function of words that rely on one another within a directed dependency parse, or links between extracted Subjects, Verbs and Objects, or nouns and the adjectives that modify them (or verbs and the adverbs that modify *them*). Rendering words linked as a network or discrete topology allows us to take advantage of the wide range of metrics and models developed for network analysis. These include measurement of network centrality, density and modularity, \"block modeling\" structurally equivalent relationships, andsophisticated graphical renderings of networks or network partitions that allow us to visually interrogate their structure and complexity.\n",
    "\n",
    "For this notebook we will use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "\n",
    "#This will be doing most of the work\n",
    "import networkx as nx\n",
    "\n",
    "import nltk #For POS tagging\n",
    "import sklearn #For generating some matrices\n",
    "import pandas #For DataFrames\n",
    "import numpy as np #For arrays\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "import seaborn #Makes the plots look nice\n",
    "import scipy #Some stats\n",
    "\n",
    "import pickle #if you want to save layouts\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to networks in *networkx*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will primarily be dealing with graphs in this notebook, so lets first go over how to use them.\n",
    "\n",
    "To start with lets create an undirected graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7fe8acd87940>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = nx.Graph()\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add nodes. These are all named, like entries in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_node(1)\n",
    "g.add_node(2)\n",
    "g.add_node(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 3 vertices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we want to get more information about the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 3\n",
      "Number of edges: 0\n",
      "Average degree:   0.0000\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give nodes properties, like name or type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.nodes[1]['type'] = 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'NN'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.nodes[2]['name'] = 'dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'dog'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still pretty boring...\n",
    "\n",
    "Lets add a couple of edges. Notice that we use the ids, but not any of the properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 4\n",
      "Number of edges: 4\n",
      "Average degree:   2.0000\n"
     ]
    }
   ],
   "source": [
    "g.add_edges_from([(1, 2), (2, 3), (3, 1), (1,4)])\n",
    "print(nx.info(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the summary has changed. Moreover, there's one additional node, because we asked for an edge to 4.\n",
    "\n",
    "We can also give the edges properties like weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edges[1, 2]['weight'] = 2\n",
    "g.edges[1, 4]['weight'] = 2\n",
    "g.edges[1, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our graph now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8acd87fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw_networkx(g)\n",
    "plt.savefig(\"test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very exciting :-).\n",
    "\n",
    "There are many things to do with the graph once we have created it, some of which we will explore here.\n",
    "\n",
    "First let's load some data, like the Grimmer Senate press releases corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>download_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>targetSenator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>01Apr2005Kennedy14.txt</td>\n",
       "      <td>raw/Kennedy/01Apr2005Kennedy14.txt</td>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>01Aug2005Kennedy12.txt</td>\n",
       "      <td>raw/Kennedy/01Aug2005Kennedy12.txt</td>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>01Aug2006Kennedy10.txt</td>\n",
       "      <td>raw/Kennedy/01Aug2006Kennedy10.txt</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        download_url  \\\n",
       "0  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "1  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "2  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "\n",
       "                                            html_url                    name  \\\n",
       "0  https://github.com/lintool/GrimmerSenatePressR...  01Apr2005Kennedy14.txt   \n",
       "1  https://github.com/lintool/GrimmerSenatePressR...  01Aug2005Kennedy12.txt   \n",
       "2  https://github.com/lintool/GrimmerSenatePressR...  01Aug2006Kennedy10.txt   \n",
       "\n",
       "                                 path  \\\n",
       "0  raw/Kennedy/01Apr2005Kennedy14.txt   \n",
       "1  raw/Kennedy/01Aug2005Kennedy12.txt   \n",
       "2  raw/Kennedy/01Aug2006Kennedy10.txt   \n",
       "\n",
       "                                                text targetSenator  \n",
       "0           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...       Kennedy  \n",
       "1           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...       Kennedy  \n",
       "2           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...       Kennedy  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesDF = pandas.read_csv('../data/senReleasesTraining.csv', index_col = 0)\n",
    "senReleasesDF[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be extracting sentences, as well as tokenizing and stemming. (You should be able to do this in your sleep now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesDF['tokenized_sents'] = senReleasesDF['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "senReleasesDF['normalized_sents'] = senReleasesDF['tokenized_sents'].apply(lambda x: [lucem_illud.normalizeTokens(s, stopwordLst = lucem_illud.stop_words_basic, stemmer = lucem_illud.stemmer_basic) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by looking at words that co-occur in the same sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCooccurrence(sentences, makeMatrix = False):\n",
    "    words = set()\n",
    "    for sent in sentences:\n",
    "        words = set(sent)\n",
    "    wordLst = list(words)\n",
    "    wordIndices = {w: i for i, w in enumerate(wordLst)}\n",
    "    wordCoCounts = {}\n",
    "    #consider a sparse matrix if memory becomes an issue\n",
    "    coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\n",
    "    for sent in sentences:\n",
    "        for i, word1 in enumerate(sent):\n",
    "            word1Index = wordIndices[word1]\n",
    "            for word2 in sent[i + 1:]:\n",
    "                coOcMat[word1Index][wordIndices[word2]] += 1\n",
    "    if makeMatrix:\n",
    "        return coOcMat, wordLst\n",
    "    else:\n",
    "        coOcMat = coOcMat.T + coOcMat\n",
    "        g = nx.convert_matrix.from_numpy_matrix(coOcMat)\n",
    "        g = nx.relabel_nodes(g, {i : w for i, w in enumerate(wordLst)})\n",
    "        return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, build a graph based on word cooccurences in the first 100 press releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'contact'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-50c783fbc6ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordCooccurrence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msenReleasesDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_sents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-767d50293221>\u001b[0m in \u001b[0;36mwordCooccurrence\u001b[0;34m(sentences, makeMatrix)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mword1Index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordIndices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mcoOcMat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword1Index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwordIndices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmakeMatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcoOcMat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordLst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'contact'"
     ]
    }
   ],
   "source": [
    "g = wordCooccurrence(senReleasesDF['normalized_sents'][:100].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of vertices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A part of the adjacency matrix of cleaned word by press releases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.to_numpy_matrix(g)[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the graph and read it later, although this is slow if there are many edges or nodes, so we will filter first, as we will demonstrate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nx.write_graphml(g, '../data/Obama_words.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can build graphs starting with a two-mode network. Let's again use the document-word frequency matrix that we used in week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenlist = nltk.word_tokenize(text)\n",
    "    normalized = lucem_illud.normalizeTokens(tokenlist, stopwordLst = lucem_illud.stop_words_basic, stemmer = lucem_illud.stemmer_basic)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senVectorizer = sklearn.feature_extraction.text.CountVectorizer(tokenizer = tokenize)\n",
    "senVects_incidence = senVectorizer.fit_transform(senReleasesDF['text'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senVects_incidence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to turn the incidence matrix into a network. Note the use of bipartite as an attribute--this is how you need to tell networkx the graph is bipartite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_2mode = nx.Graph()\n",
    "\n",
    "#define all the nodes\n",
    "g_2mode.add_nodes_from((senVectorizer.get_feature_names()[i] for i in range(senVects_incidence.shape[1])), bipartite = 'word')\n",
    "g_2mode.add_nodes_from(range(senVects_incidence.shape[0]), bipartite = 'doc')\n",
    "\n",
    "#add all the edges\n",
    "g_2mode.add_edges_from(((d, senVectorizer.get_feature_names()[w], {'weight' : senVects_incidence[d, w]}) for d, w in zip(*senVects_incidence.nonzero())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(g_2mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very popular layout algorithm for visualizing graphs is the Fruchterman-Reingold Algorithm (or spring layout), which uses a physical metaphor for lay-out. Nodes repel one another, and edges draw connected elements together like springs. The algorithm attempts to minimize the energy in such a system. For a large graph, however, the algorithm is computational demanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the bipartite network with a quick spring layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(g_2mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no filtering, this will not bring insight (see below). If we want even faster computation and tunable visualizations, check out [Pajek](http://mrvar.fdv.uni-lj.si/pajek/) or [gephi](https://gephi.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A two-mode network can be easily transformed into two one-mode network, enabling words to be connected to other words via the number of documents that share them, or documents to be connected to other documents via the words they share:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contractNetwork(g, targetType):\n",
    "    g_mono = nx.Graph()\n",
    "    g_mono.add_nodes_from(((n, d) for n, d in g_2mode.nodes(data = True) if d['bipartite'] == targetType))\n",
    "    \n",
    "    for n_outside in (n for n, d in g_2mode.nodes(data = True) if d['bipartite'] != targetType):\n",
    "        neighbors = list((n for n in g.neighbors(n_outside) if g.nodes[n]['bipartite'] == targetType))\n",
    "        for i, n1 in enumerate(neighbors):\n",
    "            for n2 in neighbors[i+1:]:\n",
    "                try:\n",
    "                    g_mono.edges[n1, n2]['weight'] += 1\n",
    "                except KeyError:\n",
    "                    g_mono.add_edge(n1, n2, weight = 1)\n",
    "    return g_mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gDoc = contractNetwork(g_2mode, 'doc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the document-to-document network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(gDoc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a visualization. It is not surprising that almost every document is connected to every other. We can use edge weight to distinguish document distance (modeled as attraction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = nx.spring_layout(gDoc, k = 1/3, weight='weight', iterations= 50)\n",
    "nx.draw(gDoc, pos = layout, labels = {n:n for n in gDoc.nodes()}) #Getting labels is a bit annoying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets draw the graph with high and low weight edges distinguished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wMedian = np.median([d['weight'] for n1, n2, d in gDoc.edges(data = True)])\n",
    "edgesHigh = [(n1, n2) for n1, n2, d in gDoc.edges(data = True) if d['weight'] > wMedian]\n",
    "edgesLow = [(n1, n2) for n1, n2, d in gDoc.edges(data = True) if d['weight'] <= wMedian]\n",
    "nx.draw(gDoc, pos = layout, labels = {n:n for n in gDoc.nodes()}, edgelist = edgesLow, style='dotted', width=.5)\n",
    "nx.draw(gDoc, pos = layout, nodelist=None, edgelist = edgesHigh, width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see why 53 and 63 are on the outside, while 39 and 23 are at the center. We can look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(senReleasesDF.iloc[63]['text']),\"words:\",\"\\n\",senReleasesDF.iloc[63]['text'].strip().replace('  ', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(senReleasesDF.iloc[23]['text']),\"words:\",\"\\n\",senReleasesDF.iloc[23]['text'].strip().replace('  ', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah...those documents with the **most** words are unsprisingly the most central in this simple document network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn it around and look at the word-to-word network by documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gWord = contractNetwork(g_2mode, 'word')\n",
    "print(nx.info(gWord))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reduce the number of words to a manageable size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wMean = np.mean([d['weight'] for n1, n2, d in gWord.edges(data = True)])\n",
    "wMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to return to the sentence cooccurence graph, as it suggests many more meaningful (more local) associations. But without filtering, it is too large. Let's first drop all the edges with weight below 1250, then drop all the isolates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.remove_edges_from([(n1, n2) for n1, n2, d in g.edges(data = True) if d['weight'] <= 1250])\n",
    "g.remove_nodes_from(list(nx.isolates(g))) #since we are changing the graph list() evaluates the isolates first\n",
    "giant = max(nx.connected_component_subgraphs(g), key=len) # keep just the giant connected component\n",
    "print(nx.info(giant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit smaller now, Now, let's visualize it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = nx.spring_layout(giant, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=.2, \n",
    "        alpha = .9, \n",
    "        node_size = 100,\n",
    "        node_color = \"xkcd:light red\",\n",
    "        edge_color='xkcd:sky blue') #Getting labels is a bit annoying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove those words that show up in many documents...those with many connections (note that this has a similar effect to selecting those with a high tf.idf score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant.remove_nodes_from([n for n in giant.nodes if len(set(giant.neighbors(n))) >= 2000]) \n",
    "giant.remove_nodes_from(list(nx.isolates(giant)))\n",
    "print(nx.info(giant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = nx.spring_layout(giant, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=.2, \n",
    "        alpha = .9, \n",
    "        node_size = 100,\n",
    "        node_color = \"xkcd:light red\",\n",
    "        edge_color='xkcd:sky blue') #Getting labels is a bit annoying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue to trim globally to investigate the structure of words at alternative slices of network density. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can find cliques, or completely connected sets of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.clique.number_of_cliques(giant)['presid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(', '.join(max(nx.clique.find_cliques(giant), key = lambda x: len(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at a subgraph of the network, those nodes that are within 1 or 2 network steps of 'america'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "americanNeighbors = giant.neighbors('american')\n",
    "g_american = giant.subgraph(americanNeighbors)\n",
    "print(nx.info(g_american))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_amer = nx.spring_layout(g_american, weight='weight', iterations= 100, k = .3)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxWeight = max((d['weight'] for n1, n2, d in g_american.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in g_american.edges(data = True)))\n",
    "nx.draw(g_american, ax = ax, pos = layout_amer, labels = {n:n for n in g_american.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in g_american.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:sky blue',\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "americanNeighbors = gWord.neighbors('american')\n",
    "americanNeighborsPlus1 = set(americanNeighbors)\n",
    "for n in americanNeighbors:\n",
    "    americanNeighborsPlus1 |= set(giant.neighbors(n))\n",
    "    \n",
    "#for x in americanNeighborsPlus1:\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_american2 = giant.subgraph(americanNeighborsPlus1)\n",
    "print(nx.info(g_american2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_amer = nx.spring_layout(g_american2, weight='weight', iterations= 100, k = .3)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "centralities_amer = nx.eigenvector_centrality(g_american2)\n",
    "maxC = max(centralities_amer.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in g_american2.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in g_american2.edges(data = True)))\n",
    "nx.draw(g_american2, ax = ax, pos = layout_amer, labels = {n:n for n in g_american2.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in g_american2.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:sky blue',\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that render some reasonable networks to meaningfully characterize the structure of words and documents (or subdocuments like chapters or paragraphs) from your corpus. What are useful filters and thresholds and what semantic structures do they reveal that given insight into the social world and social game inscribed in your corpus? Interrogate interesting subgraphs of your network and detail what they reveal about the semantic space involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Network Statistics\n",
    "We can often gain more insight into our semantic network through statistics that describe the positions of words within it.\n",
    "\n",
    "We begin with measures of centrality. The concept of centrality is that some nodes (words or documents) are more *central* to the network than others. The most straightforward is the notion of degree centrality: those nodes that have the highest number of connections are the most central. Here our measure normalizes the number of connections by those with the most connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcentralities = nx.degree_centrality(giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcentralities['senat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(dcentralities.items(), key = lambda x : x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can color and size the nodes by betweenness centrality, it's much faster to redraw since we aren't redoing the layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxC = max(dcentralities.items(), key = lambda x : x[1])[1]\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=.2, \n",
    "        alpha = .9, \n",
    "        node_size = 100,\n",
    "        node_color = [dcentralities[n] / maxC for n in giant.nodes],\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize the graph involes the use of size to represent degree centrality and edge weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxC = max(dcentralities.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in gWord.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [dcentralities[n] / maxC for n in giant.nodes],\n",
    "        node_size = [dcentralities[n] / maxC * 200 for n in giant.nodes],\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distrubution of degree centrality is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(dcentralities.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top and bottom ten words in terms of degree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dcentralities.items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dcentralities.items(), key = lambda x: x[1], reverse = True)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider another very different measure, which is *betweenness* centrality. Betweenness centrality distinguishes nodes that require the most shortest pathways between all other nodes in the network. Semantically, words with a high betweenness centrality may link distinctive domains, rather than being \"central\" to any one. In other words, high *betweenness centrality* nodes may not have the highest *degree centrality*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centralities = nx.betweenness.betweenness_centrality(giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities['senat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(centralities.items(), key = lambda x : x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can color and size the nodes by betweenness centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxC = max(centralities.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in giant.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [centralities[n] / maxC for n in giant.nodes],\n",
    "        node_size = [centralities[n] / maxC * 200 for n in giant.nodes],\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distrubution of betweenness centrality is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(centralities.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exponential distrubution, but you might need to add more nodes to see it clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top ten words in terms of betweenness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dcentralities.items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here it appears that \"health\"/\"care\"/\"cost\"/\"medicar\", \"famili\"/\"children\", and \"bill\" are key concepts that connect others in the broader network. This is interesting in that they seem to be domain-specific rather than linking words like \"service\" and \"system\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are words lower down?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dcentralities.items(), key = lambda x: x[1], reverse = True)[50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at closeness centrality, or the average Euclidean or path distance between a node and all others in the network. A node with the highest closeness centrality is most likely to send a signal with the most coverage to the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centralities = nx.closeness_centrality(giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxC = max(centralities.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in giant.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [centralities[n] / maxC for n in giant.nodes],\n",
    "        node_size = [centralities[n] / maxC * 200 for n in giant.nodes],\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top and bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(centralities.items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(centralities.items(), key = lambda x: x[1], reverse = True)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or eignvector centrality, an approach that weights degree by the centrality of those to whom one is tied (and the degree to whom they are tied, etc.) In short, its an $n$th order degree measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centralities = nx.eigenvector_centrality(giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxC = max(centralities.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in giant.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [centralities[n] / maxC for n in giant.nodes],\n",
    "        node_size = [centralities[n] / maxC * 200 for n in giant.nodes],\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the colors reveal a much more graduate distribution here. Let's look at it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(centralities.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top and bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(centralities.items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(centralities.items(), key = lambda x: x[1], reverse = True)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now filter our network by a centrality measure. Let's define a function and experiment with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterWords(G, minWeight = 3, filter_ = \"betweenness\", rule = \"number\", value_of_rule = 200):\n",
    "    \"\"\"Function to filter network by degree centrality measures\"\"\"\n",
    "    G = G.copy()\n",
    "    try:\n",
    "        G.remove_edges_from([(n1,n2) for n1, n2, d in G.edges(data = True) if d['weight'] < minWeight])\n",
    "    except:\n",
    "        print(\"weight might be missing from one or more edges\")\n",
    "        raise\n",
    "    if filter_ ==\"betweenness\":\n",
    "        index = nx.betweenness_centrality(G) #betweeness centrality score\n",
    "    elif filter_ == \"closeness\":\n",
    "        index = nx.closeness_centrality(G) #closeness centrality score\n",
    "    elif filter_ == \"eigenvector\":\n",
    "        index = nx.eigenvector_centrality(G) #eigenvector centrality score\n",
    "    elif filter_ == \"degree\":\n",
    "        index = nx.degree_centrality(G) #degree centrality score\n",
    "    else:\n",
    "        raise ValueError(\"wrong filter paremeter, should be: betweenness/closeness/eigenvector\")    \n",
    "        \n",
    "    if rule=='number':# if filter by limiting the total number of nodes \n",
    "        \n",
    "        sorted_index = sorted(index.items(), key=lambda x:x[1], reverse=True)\n",
    "        value_of_rule = np.min([value_of_rule, len(G.nodes)])\n",
    "        \n",
    "        nodes_remain = {}\n",
    "        for word, centr in sorted_index[:value_of_rule]:\n",
    "            nodes_remain[word] = centr\n",
    "        G.remove_nodes_from([n for n in index if n not in nodes_remain])\n",
    "        print (\"Total number of nodes(after filtering) in the graph is %s\" % len(G))\n",
    "        return G\n",
    "    \n",
    "    if rule=='above':# if filter by limiting the min value of centrality\n",
    "        value_of_rule = np.max([float(value_of_rule),0])\n",
    "        G.remove_nodes_from([n for n in index if index[n] >=value_of_rule])\n",
    "        print (\"Total number of nodes(after filtering) in the graph is %s\" % len(G))\n",
    "        return G\n",
    "    \n",
    "    if rule=='below':# if filter by limiting the max value of centrality\n",
    "        value_of_rule = np.max([float(value_of_rule),0])\n",
    "        G.remove_nodes_from([n for n in index if index[n] <=value_of_rule])\n",
    "        print (\"Total number of nodes(after filtering) in the graph is %s\" % len(G))\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant_filtered30 = filterWords(giant, minWeight=3, filter_='betweenness', rule='number', value_of_rule=25)\n",
    "print(nx.info(giant_filtered30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_giant_filtered30 = nx.spring_layout(giant_filtered30, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "centralities_giant30 = nx.betweenness_centrality(giant_filtered30)\n",
    "maxC = max(centralities_giant30.items(), key = lambda x : x[1])[1]\n",
    "nx.draw(giant_filtered30, ax = ax, pos = layout_giant_filtered30, labels = {n: n for n in giant_filtered30.nodes()},\n",
    "        alpha = .9, \n",
    "        width = .5,\n",
    "        node_color = [centralities_giant30[n] / maxC for n in giant_filtered30.nodes],\n",
    "        node_size = [centralities_giant30[n] / maxC * 100 for n in giant_filtered30.nodes],\n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:medium blue',\n",
    "        cmap = plt.get_cmap('plasma'),\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at global statistics, like the density of a network, defined as the number of actual edges divided by the total number of possible edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(giant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the average degree per node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([v for w,v in nx.degree(giant)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diameter calculates the average distance between any two nodes in the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.diameter(giant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that calculate different kinds of centrality for distinct words or documents in a network composed from your corpus of interest. Which type of words tend to be most and least central? Can you identify how different centrality measures distinguish different kind of words in your corpus? What do these patterns suggest about the semantic content and structure of your documents? Finally, calculate global measure for your network(s) and discuss what they reveal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS based networks\n",
    "\n",
    "Now let's look at links between specific parts of speech within a network.\n",
    "\n",
    "For this we will be using the `nltk` POS facilities instead of the Stanford ones. These are much faster, but also somewhat less accurate. (You get what you *pay* for in computational power).\n",
    "\n",
    "Let's look at nouns co-occurring in sentences using the top 10 (by score) reddit posts on thread topics we have explored in prior sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditDF = pandas.read_csv('../data/reddit.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditTopScores = redditDF.sort_values('score')[-100:]\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "redditTopScores.index = range(len(redditTopScores) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "redditTopScores[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll normalize the tokens through stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['normalized_sents'] = redditTopScores['sentences'].apply(lambda x: [lucem_illud.normalizeTokens(s, stopwordLst = None, stemmer = lucem_illud.stemmer_basic) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posCooccurrence(sentences, *posType, makeMatrix = False):\n",
    "    words = set()\n",
    "    reducedSents = []\n",
    "    #Only using the first kind of POS for each word\n",
    "    wordsMap = {}\n",
    "    for sent in sentences:\n",
    "        s = [(w, t) for w, t in nltk.pos_tag(sent) if t in posType]\n",
    "        for w, t in s:\n",
    "            if w not in wordsMap:\n",
    "                wordsMap[w] = t\n",
    "        reducedSent = [w for w, t in s]\n",
    "        words |= set(reducedSent)\n",
    "        reducedSents.append(reducedSent)\n",
    "    wordLst = list(words)\n",
    "    wordIndices = {w: i for i, w in enumerate(wordLst)}\n",
    "    wordCoCounts = {}\n",
    "    #consider a sparse matrix if memory becomes an issue\n",
    "    coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\n",
    "    for sent in reducedSents:\n",
    "        for i, word1 in enumerate(sent):\n",
    "            word1Index = wordIndices[word1]\n",
    "            for word2 in sent[i + 1:]:\n",
    "                coOcMat[word1Index][wordIndices[word2]] += 1\n",
    "    if makeMatrix:\n",
    "        return coOcMat, wordLst\n",
    "    else:\n",
    "        coOcMat = coOcMat.T + coOcMat\n",
    "        g = nx.convert_matrix.from_numpy_matrix(coOcMat)\n",
    "        g = nx.relabel_nodes(g, {i : w for i, w in enumerate(wordLst)})\n",
    "        for w in g.nodes:\n",
    "            g.nodes[w]['bipartite'] = wordsMap[w]\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gNN = posCooccurrence(redditTopScores['normalized_sents'].sum(), 'NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(gNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a bit too large to effectively visualize, so let's remove the vertices with degree less than or equal to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gNN.remove_nodes_from([n for n in gNN.nodes if len(set(gNN.neighbors(n))) <= 100]) \n",
    "print(nx.info(gNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And low weight edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gNN.remove_edges_from([(n1, n2) for n1, n2, d in gNN.edges(data = True) if d['weight'] <= 2])\n",
    "print(nx.info(gNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_nn = nx.spring_layout(gNN, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "centralities_nn = nx.eigenvector_centrality(gNN)\n",
    "maxC = max(centralities_nn.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in gNN.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in gNN.edges(data = True)))\n",
    "nx.draw(gNN, ax = ax, pos = layout_nn, labels = {n: n for n in gNN.nodes()},\n",
    "        #width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in gNN.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [centralities_nn[n] / maxC for n in gNN.nodes],\n",
    "        node_size = [centralities_nn[n] / maxC * 100 for n in gNN.nodes],\n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:medium blue',\n",
    "        cmap = plt.get_cmap('plasma'),\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is an interesting pattern; everyone is talking about themselves (\"I...this\", \"I...that\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to look at noun-verb pairs instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gNV = posCooccurrence(redditTopScores['normalized_sents'].sum(), 'NN', 'VB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gNV` has co-occurrences between nouns and nouns as well as between verbs and verbs. Let's remove these and make it purely about noun and verb combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(gNV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gNV.remove_edges_from([(n1,n2) for n1,n2,d in gNV.edges(data = True) if gNV.nodes[n1]['bipartite'] == gNV.nodes[n2]['bipartite']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(gNV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping low weight edges and low degree vertices gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gNV.remove_edges_from([(n1, n2) for n1, n2, d in gNV.edges(data = True) if d['weight'] <= 2])\n",
    "gNV.remove_nodes_from([n for n in gNV.nodes if len(set(gNV.neighbors(n))) <= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(gNV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_nn = nx.spring_layout(gNV, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "centralities_nv = nx.eigenvector_centrality(gNV)\n",
    "maxC = max(centralities_nv.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in gNV.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in gNV.edges(data = True)))\n",
    "nx.draw(gNV, ax = ax, pos = layout_nn, labels = {n: n for n in gNV.nodes()},\n",
    "        #width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in gNN.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [centralities_nv[n] / maxC for n in gNV.nodes],\n",
    "        node_size = [centralities_nv[n] / maxC * 100 for n in gNV.nodes],\n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:medium blue',\n",
    "        cmap = plt.get_cmap('plasma'),\n",
    "       ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create an \"ego network\" surrounding a single (important) word, as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_i = gNV.subgraph(['i'] + list(gNV.neighbors('i')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(g_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(g_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just nodes connected to a vertex, we can find all those connected to it within 2 hops, lets look at 'stori' for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storyNeighbors = gNV.neighbors('stori')\n",
    "set(storyNeighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storyNeighbors = set(gNV.neighbors('stori'))\n",
    "storyNeighborsPlus1 = set(storyNeighbors)\n",
    "for n in storyNeighbors:\n",
    "    storyNeighborsPlus1 |= set(gNV.neighbors(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gNV_story = gNV.subgraph(storyNeighborsPlus1)\n",
    "print(nx.info(gNV_story))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a mid-sized network, but we can interrogate it intelligently by computing some statistics. Degree centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nx.degree_centrality(gNV_story).items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or eigenvector centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nx.eigenvector_centrality(gNV_story).items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that 'stori' isn't even in the top 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_story = nx.spring_layout(gNV_story, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "nx.draw(gNV_story, ax = ax, pos = layout_story, labels = {n: n for n in gNV_story.nodes()},\n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:medium blue',\n",
    "       ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"I\" is still in the middle. These are stories about the self."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a noun-adjective network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gNJ = posCooccurrence(redditTopScores['normalized_sents'][:100].sum(), 'NN', 'JJ')\n",
    "print(nx.info(gNJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By filtering by a centrality measure we can get a more 'central' set of nodes instead of just the most connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gNJ_filtered200 = filterWords(gNJ, minWeight=3, filter_='betweenness', rule='number', value_of_rule=25)\n",
    "print(nx.info(gNJ_filtered200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_NJ_filtered200 = nx.spring_layout(gNJ_filtered200, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "centralities_nj200 = nx.betweenness_centrality(gNJ_filtered200)\n",
    "maxC = max(centralities_nj200.items(), key = lambda x : x[1])[1]\n",
    "nx.draw(gNJ_filtered200, ax = ax, pos = layout_NJ_filtered200, labels = {n: n for n in gNJ_filtered200.nodes()},\n",
    "        alpha = .9, \n",
    "        width = .5,\n",
    "        node_color = [centralities_nj200[n] / maxC for n in gNJ_filtered200.nodes],\n",
    "        node_size = [centralities_nj200[n] / maxC * 100 for n in gNJ_filtered200.nodes],\n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:medium blue',\n",
    "        cmap = plt.get_cmap('plasma'),\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A truly self-centered network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that construct at least two different networks comprising different combinations of word types, linked by different syntactic structures, which illuminate your corpus and the dynamics you are interested to explore. Graph these networks or subnetworks within them. What are relationships that are meaningful? \n",
    "\n",
    "<span style=\"color:red\">***Stretch***: Graph some word-centered \"ego-networks\" with words one link away, two links away, and three links away (we only did up to two links away above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactional influence\n",
    "\n",
    "In a recent paper by Fangjian Guo, Charles Blundell, Hanna Wallach, and Katherine Heller entitled [\"The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation\"](https://arxiv.org/pdf/1411.2674.pdf), the authors develop a method to estimate the influence of one speaker on another in order to estimate a kind of interpersonal influence network. Here we walk through this method, which relies on a kind of point process called a Hawkes process that estimate the influence of one point on another. Specifically, what they estimate is the degree to which one actor to an interpersonal interaction engaged in \"accomodation\" behaviors relative to the other, generating a directed edge from the one to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's look at the output of their analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_name = '12-angry-men'   #example datasets: \"12-angry-men\" or \"USpresident\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = '../data/Bayesian-echo/results/{}/'.format(example_name)\n",
    "if not os.path.isdir(result_path):\n",
    "    raise ValueError('Invalid example selected, only \"12-angry-men\" or \"USpresident\" are avaliable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_info = pandas.read_table(result_path + 'meta-info.txt',header=None)\n",
    "df_log_prob = pandas.read_csv(result_path + \"SAMPLE-log_prior_and_log_likelihood.txt\",delim_whitespace=True) #log_prob samples\n",
    "df_influence = pandas.read_csv(result_path + 'SAMPLE-influence.txt',delim_whitespace=True) # influence samples\n",
    "df_participants = pandas.read_csv(result_path + 'cast.txt', delim_whitespace=True)\n",
    "person_id = pandas.Series(df_participants['agent.num'].values-1,index=df_participants['agent.name']).to_dict()\n",
    "print()\n",
    "print ('Person : ID')\n",
    "person_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDensity(df):\n",
    "    data = df#_log_prob['log.prior']\n",
    "    density = scipy.stats.gaussian_kde(data)\n",
    "    width = np.max(data) - np.min(data)\n",
    "    xs = np.linspace(np.min(data)-width/5, np.max(data)+width/5,600)\n",
    "    density.covariance_factor = lambda : .25\n",
    "    density._compute_covariance()\n",
    "    return xs, density(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot MCMC (Markov Monte Carlo) trace and the density of log-likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,10])\n",
    "\n",
    "plt.subplot(4,2,1)\n",
    "plt.plot(df_log_prob['log.prior'])\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Trace of log.prior')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "x,y = getDensity(df_log_prob['log.prior'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.prior')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "plt.plot(df_log_prob['log.likelihood'])\n",
    "plt.title('Trace of log.likelihood')\n",
    "plt.xlabel('Iterations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "x,y = getDensity(df_log_prob['log.likelihood'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.likelihood')\n",
    "\n",
    "plt.subplot(4,2,5)\n",
    "plt.plot(df_log_prob['log.likelihood.test.set'])\n",
    "plt.title('Trace of log.likelihood.test.set')\n",
    "plt.xlabel('Iterations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(4,2,6)\n",
    "x,y = getDensity(df_log_prob['log.likelihood.test.set'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.likelihood.test.set')\n",
    "\n",
    "plt.subplot(4,2,7)\n",
    "plt.plot(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n",
    "plt.title('Trace of log.prob')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "plt.subplot(4,2,8)\n",
    "x,y = getDensity(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.prob')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the influence matrix between participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = int(np.sqrt(len(df_influence.columns))) #number of participants\n",
    "id_person = {}\n",
    "for p in person_id:\n",
    "    id_person[person_id[p]]=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getmatrix(stacked,A):\n",
    "    influence_matrix = [[0 for i in range(A)] for j in range(A)]\n",
    "    for row in stacked.iteritems():\n",
    "        from_ = int(row[0].split('.')[1])-1\n",
    "        to_ = int(row[0].split('.')[2])-1\n",
    "        value = float(row[1])\n",
    "        influence_matrix[from_][to_]=value\n",
    "    df_ = pandas.DataFrame(influence_matrix) \n",
    "    \n",
    "    df_ =df_.rename(index = id_person)\n",
    "    df_ =df_.rename(columns = id_person)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = df_influence.mean(axis=0)\n",
    "df_mean = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.std(axis=0)\n",
    "df_std = getmatrix(stacked,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "seaborn.heatmap(df_mean, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n",
    "print('MEAN of influence matrix (row=from, col=to)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "seaborn.heatmap(df_std, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n",
    "print('SD of influence matrix (row=from, col=to)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot of total influences sent/received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_std = {} #sd of total influence sent\n",
    "reciever_std = {} #sd of total influence recieved\n",
    "for i in range(A):\n",
    "    reciever_std[id_person[i]] = df_influence[df_influence.columns[i::A]].sum(axis=1).std()\n",
    "    sender_std[id_person[i]] = df_influence[df_influence.columns[i*A:(i+1)*A:]].sum(axis=1).std()\n",
    "\n",
    "sent = df_mean.sum(axis=1) #mean of total influence sent\n",
    "recieved =df_mean.sum(axis=0) #mean of total influence recieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total influence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\t\\tTotal linguistic influence sent/received \")\n",
    "ax.fig = plt.figure(figsize=[np.min([A,20]),6])\n",
    "\n",
    "plt.grid()\n",
    "wd=0.45\n",
    "ii=0\n",
    "for p in sender_std:\n",
    "    plt.bar(person_id[p],sent.loc[p],width=wd,color='red',alpha=0.6,label = \"Sent\" if ii == 0 else \"\")\n",
    "    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]+sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n",
    "    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]-sender_std[p],sent.loc[p]-sender_std[p]],color='k')\n",
    "    plt.plot([person_id[p],person_id[p]],[sent.loc[p]-sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n",
    "    ii+=1\n",
    "ii=0\n",
    "for p in reciever_std:\n",
    "    plt.bar(person_id[p]+wd,recieved.loc[p],width=wd,color='blue',alpha=0.4,label = \"Received\" if ii == 0 else \"\")\n",
    "    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]+reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n",
    "    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]-reciever_std[p],recieved.loc[p]-reciever_std[p]],color='k')\n",
    "    plt.plot([person_id[p]+wd,person_id[p]+wd],[recieved.loc[p]-reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n",
    "    ii+=1\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.7))\n",
    "plt.xticks([i+0.25 for i in range(A)],list(zip(*sorted(id_person.items())))[1])\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('speaker',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Influence Network!\n",
    "\n",
    "You can visualize any of the influence matrices above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using networkx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drawNetwork(df,title):\n",
    "    fig = plt.figure(figsize=[8,8])\n",
    "    G = nx.DiGraph()\n",
    "    for from_ in df.index:\n",
    "        for to_ in df.columns:\n",
    "            G.add_edge(from_,to_,weight = df.loc[from_][to_])\n",
    "            \n",
    "    pos = nx.spring_layout(G,k=0.55,iterations=20)\n",
    "    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    weights = np.array(weights)\n",
    "    #weights = weights*weights\n",
    "    weights = 6*weights/np.max(weights)\n",
    "    print(title)\n",
    "    \n",
    "    edge_colors=20*(weights/np.max(weights))\n",
    "    edge_colors = edge_colors.astype(int)\n",
    "#     nx.draw_networkx_nodes(G,pos,node_size=1200,alpha=0.7,node_color='#99cef7')\n",
    "#     nx.draw_networkx_edges(G,pos,edge_color=edge_colors)\n",
    "#     nx.draw_networkx_labels(G,pos,font_weight='bold')\n",
    "    nx.draw(G,pos,with_labels=True, font_weight='bold',width=weights,\\\n",
    "            edge_color=255-edge_colors,node_color='#99cef7',node_size=1200,\\\n",
    "            alpha=0.75,arrows=True,arrowsize=20)\n",
    "    return edge_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quantile influence matrices for 25%, 50%, 75% quantile\n",
    "stacked = df_influence.quantile(0.25)\n",
    "df_q25 = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.quantile(0.5)\n",
    "df_q50 = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.quantile(0.75)\n",
    "df_q75 = getmatrix(stacked,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_mean = drawNetwork(df_mean,'Mean Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_q25 = drawNetwork(df_q25,'25 Quantile Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_q75 = drawNetwork(df_q75,'75 Quantile Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lucem_illud\n",
    "import pandas\n",
    "def fakeEnglish(length):\n",
    "    listd=['a','b','c','d','e','f','g','s','h','i','j','k','l']\n",
    "    return ''.join(np.random.choice(listd,length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your own dataset should contains 4 columns (with the same column names) as the artificial one below:\n",
    "\n",
    "- name: name of the participant\n",
    "- tokens: a list of tokens in one utterance\n",
    "- start: starting time of utterance (unit doesn't matter, can be 'seconds','minutes','hours'...)\n",
    "- end: ending time of utterance (same unit as start)\n",
    "\n",
    "There is no need to sort data for the moment.\n",
    "\n",
    "Below, we generate a fake collection of data from \"Obama\", \"Trump\", \"Clinton\"...and other recent presidents. You can either create your own simulation OR (better), add real interactional data from a online chat forum, comment chain, or transcribed from a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script= []\n",
    "language = 'eng' #parameter, no need to tune if using English, accept:{'eng','chinese'}\n",
    "role = 'Adult' #parameter, no need to tune \n",
    "\n",
    "for i in range(290):\n",
    "    dt = []\n",
    "    dt.append(np.random.choice(['Obama','Trump','Clinton','Bush','Reagan','Carter','Ford','Nixon','Kennedy','Roosevelt']))\n",
    "    faketokens = [fakeEnglish(length = 4) for j in range(30)]\n",
    "    dt.append(faketokens) #fake utterance\n",
    "    dt.append(i*2+np.random.random()) # start time\n",
    "    dt.append(i*2+1+np.random.random()) # end time\n",
    "    script.append(dt)\n",
    "\n",
    "df_transcript = pandas.DataFrame(script,columns=['name','tokens','start','end']) #\"start\", \"end\" are timestamps of utterances, units don't matter\n",
    "df_transcript[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data into TalkbankXML format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fname = 'USpresident.xml'  #should be .xml\n",
    "language = 'eng' \n",
    "#language = 'chinese'\n",
    "lucem_illud.make_TalkbankXML(df_transcript, output_fname, language = language )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Bayesian Echo Chamber to get estimation.\n",
    "\n",
    "- It may take a couple of hours. ( About 4-5 hours if Vocab_size=600 and sampling_time =2000)\n",
    "- Larger \"Vocab_size\" (see below) will cost more time\n",
    "- Larger \"sampling_time\" will also consume more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab_size = 90 # up to Vocab_size most frequent words will be considered, it should be smaller than the total vocab\n",
    "sampling_time = 1500  #The times of Gibbs sampling sweeps  (500 burn-in not included)\n",
    "lucem_illud.bec_run(output_fname, Vocab_size, language, sampling_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realtime output can be viewed in shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that either (1) take data from a transcribed conversation, online chat forum, comment chain or some other *timed* interaction and calculate Bayesian influence; or (2) build a simulation similar in spirit to the \"Presidents\" example above, ideally infusing it with interesting \"stylized facts\". What does that estimation and its output reveal about the relative influence of each actor on others? What does it reveal about the social game being played?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
