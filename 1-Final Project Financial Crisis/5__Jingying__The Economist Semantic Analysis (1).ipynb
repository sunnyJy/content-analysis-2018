{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\"> Final Project The Economist Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucem_illud \n",
    "\n",
    "import pandas #gives us DataFrames\n",
    "import numpy as np #For divergences/distances\n",
    "import scipy #For divergences/ distances/ hierarchical clustering and some visuals\n",
    "\n",
    "import nltk #the Natural Language Toolkit\n",
    "from nltk.corpus import stopwords #For stopwords\n",
    "\n",
    "import seaborn as sns #makes our plots look nicer\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import matplotlib.cm #Still for graphics\n",
    "import wordcloud #Makes word clouds\n",
    "\n",
    "#These are all for the cluster detection\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition\n",
    "import sklearn.metrics\n",
    "import sklearn.manifold #For a manifold plot\n",
    "\n",
    "import gensim#For topic modeling\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests #for http requests\n",
    "from requests import get\n",
    "import urllib.parse #For joining urls\n",
    "import re #for regexs\n",
    "import json #For API responses\n",
    "\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:purple\"> Week 1 & Week 2\n",
    "<span style=\"color:purple\">The corpus is saved in the file named \"ECONOMIST_New.csv\". <br>\n",
    "It has been processed with the steps discessued in Week 1 and Week 2. <br>\n",
    "\n",
    "<span style=\"color:purple\">**Step 1:** Parse the .xml file and save all the information in the data frame. <br>\n",
    "<span style=\"color:purple\">**Step 2:** Coarse grain the corpus by tokenization, normalization, stoping list, and stemming. After these basic processing, the dimensionality of the corpus is largely reduced. For instance, the word counts in Fin_econ decrease from 2304689 to 1082567; in Business decrease from 2486335 to 1172374; in Sci_tech decrease from 870651 to 402963; and in Book_art decrease from 1405589 to 649146. <br>\n",
    "<span style=\"color:purple\">**Step 3:** Focus on the word frequencies, the comparison cross sectors as well as time variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Economist = pandas.read_csv('ECONOMIST_New.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Economist['Unnamed: 0']\n",
    "Economist[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topics = ['Leaders', 'Britain', 'Europe', 'United States', 'The Americas', \n",
    "              'Middle East and Africa', 'Asia', 'Obituary',\n",
    "              'Business', 'Finance and economics', 'Science and technology', 'Books and arts']\n",
    "\n",
    "years = ['2006','2007','2008','2009','2010']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:purple\">  Frequency of \"crisis\"\n",
    "<span style=\"color:purple\"> Fig 1: Use the whole corpus <br>\n",
    "<span style=\"color:purple\"> Fig 2: Loop the 4 sectors {'Business', 'Finance and economics', 'Science and technology', 'Books and arts'} <br>\n",
    "<span style=\"color:purple\"> Fig 3: Loop the different regions/ areas {'Britain', 'Europe', 'United States', 'The Americas', 'Middle East and Africa', 'Asia'} <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCounter(wordLst):\n",
    "    wordCounts = {}\n",
    "    for word in wordLst:\n",
    "        #We usually need to normalize the case\n",
    "        wLower = word.lower()\n",
    "        if wLower in wordCounts:\n",
    "            wordCounts[wLower] += 1\n",
    "        else:\n",
    "            wordCounts[wLower] = 1\n",
    "    #convert to DataFrame\n",
    "    countsForFrame = {'word' : [], 'count' : []}\n",
    "    for w, c in wordCounts.items():\n",
    "        countsForFrame['word'].append(w)\n",
    "        countsForFrame['count'].append(c)\n",
    "    return pandas.DataFrame(countsForFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:purple\"> For Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = pandas.read_csv('Economist_Token_Neat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish year 2006 and month 1\n",
      "finish year 2006 and month 2\n",
      "finish year 2006 and month 3\n",
      "finish year 2006 and month 4\n",
      "finish year 2006 and month 5\n",
      "finish year 2006 and month 6\n",
      "finish year 2006 and month 7\n",
      "finish year 2006 and month 8\n",
      "finish year 2006 and month 9\n",
      "finish year 2006 and month 10\n",
      "finish year 2006 and month 11\n",
      "finish year 2006 and month 12\n",
      "finish year 2007 and month 1\n",
      "finish year 2007 and month 2\n",
      "finish year 2007 and month 3\n",
      "finish year 2007 and month 4\n",
      "finish year 2007 and month 5\n",
      "finish year 2007 and month 6\n",
      "finish year 2007 and month 7\n",
      "finish year 2007 and month 8\n",
      "finish year 2007 and month 9\n",
      "finish year 2007 and month 10\n",
      "finish year 2007 and month 11\n",
      "finish year 2007 and month 12\n",
      "finish year 2008 and month 1\n",
      "finish year 2008 and month 2\n",
      "finish year 2008 and month 3\n",
      "finish year 2008 and month 4\n",
      "finish year 2008 and month 5\n",
      "finish year 2008 and month 6\n",
      "finish year 2008 and month 7\n",
      "finish year 2008 and month 8\n",
      "finish year 2008 and month 9\n",
      "finish year 2008 and month 10\n",
      "finish year 2008 and month 11\n",
      "finish year 2008 and month 12\n",
      "finish year 2009 and month 1\n",
      "finish year 2009 and month 2\n",
      "finish year 2009 and month 3\n",
      "finish year 2009 and month 4\n",
      "finish year 2009 and month 5\n",
      "finish year 2009 and month 6\n",
      "finish year 2009 and month 7\n",
      "finish year 2009 and month 8\n",
      "finish year 2009 and month 9\n",
      "finish year 2009 and month 10\n",
      "finish year 2009 and month 11\n",
      "finish year 2009 and month 12\n",
      "finish year 2010 and month 1\n",
      "finish year 2010 and month 2\n",
      "finish year 2010 and month 3\n",
      "finish year 2010 and month 4\n",
      "finish year 2010 and month 5\n",
      "finish year 2010 and month 6\n",
      "finish year 2010 and month 7\n",
      "finish year 2010 and month 8\n",
      "finish year 2010 and month 9\n",
      "finish year 2010 and month 10\n",
      "finish year 2010 and month 11\n",
      "finish year 2010 and month 12\n"
     ]
    }
   ],
   "source": [
    "Economist = E\n",
    "Time_y = []\n",
    "Time_m = []\n",
    "Total_word_num = []\n",
    "Target_num = []\n",
    "Date = []\n",
    "\n",
    "target = 'crisis'\n",
    "for year in years: \n",
    "    E_year = Economist[Economist.Year == int(year)]\n",
    "    for month in range(1,13):\n",
    "        E_month = E_year[E_year.Month == month]\n",
    "        E_month = E_month.reset_index()\n",
    "        del E_month['index']\n",
    "        \n",
    "        total_word_num = 0 # monthly\n",
    "        target_num = 0\n",
    "        for i in range(0, len(E_month)-1):\n",
    "            article = E_month['Article'][i]\n",
    "            \n",
    "            article_tokens = nltk.word_tokenize(article)\n",
    "            article_countedWords = wordCounter(article_tokens)\n",
    "\n",
    "            total_word_num = total_word_num + sum(list(article_countedWords['count']))\n",
    "            if list(article_countedWords[article_countedWords.word == target]['count']) == []:\n",
    "                target_num = target_num + 0\n",
    "            else: \n",
    "                target_num = target_num + list(article_countedWords[article_countedWords.word == target]['count'])[0]\n",
    "        \n",
    "        Total_word_num.append(total_word_num)\n",
    "        Target_num.append(target_num)\n",
    "        Time_y.append(int(year))\n",
    "        Time_m.append(month)\n",
    "        Date.append(year+'-'+str(month))\n",
    "        print('finish year '+ year+' and month '+str(month))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_table_All = pandas.DataFrame({'Time_y': Time_y, 'Time_m': Time_m, \n",
    "                                   'Total_word_num': Total_word_num, target+'_num': Target_num})\n",
    "Freq_table_All = Freq_table_All[['Time_y', 'Time_m', 'Total_word_num', target+'_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_table_All.to_csv('Freq_table_crisis.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:purple\"> For Figure 2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish year 2006 and month 1\n",
      "finish year 2006 and month 2\n",
      "finish year 2006 and month 3\n",
      "finish year 2006 and month 4\n",
      "finish year 2006 and month 5\n",
      "finish year 2006 and month 6\n",
      "finish year 2006 and month 7\n",
      "finish year 2006 and month 8\n",
      "finish year 2006 and month 9\n",
      "finish year 2006 and month 10\n",
      "finish year 2006 and month 11\n",
      "finish year 2006 and month 12\n",
      "finish year 2007 and month 1\n",
      "finish year 2007 and month 2\n",
      "finish year 2007 and month 3\n",
      "finish year 2007 and month 4\n",
      "finish year 2007 and month 5\n",
      "finish year 2007 and month 6\n",
      "finish year 2007 and month 7\n",
      "finish year 2007 and month 8\n",
      "finish year 2007 and month 9\n",
      "finish year 2007 and month 10\n",
      "finish year 2007 and month 11\n",
      "finish year 2007 and month 12\n",
      "finish year 2008 and month 1\n",
      "finish year 2008 and month 2\n",
      "finish year 2008 and month 3\n",
      "finish year 2008 and month 4\n",
      "finish year 2008 and month 5\n",
      "finish year 2008 and month 6\n",
      "finish year 2008 and month 7\n",
      "finish year 2008 and month 8\n",
      "finish year 2008 and month 9\n",
      "finish year 2008 and month 10\n",
      "finish year 2008 and month 11\n",
      "finish year 2008 and month 12\n",
      "finish year 2009 and month 1\n",
      "finish year 2009 and month 2\n",
      "finish year 2009 and month 3\n",
      "finish year 2009 and month 4\n",
      "finish year 2009 and month 5\n",
      "finish year 2009 and month 6\n",
      "finish year 2009 and month 7\n",
      "finish year 2009 and month 8\n",
      "finish year 2009 and month 9\n",
      "finish year 2009 and month 10\n",
      "finish year 2009 and month 11\n",
      "finish year 2009 and month 12\n",
      "finish year 2010 and month 1\n",
      "finish year 2010 and month 2\n",
      "finish year 2010 and month 3\n",
      "finish year 2010 and month 4\n",
      "finish year 2010 and month 5\n",
      "finish year 2010 and month 6\n",
      "finish year 2010 and month 7\n",
      "finish year 2010 and month 8\n",
      "finish year 2010 and month 9\n",
      "finish year 2010 and month 10\n",
      "finish year 2010 and month 11\n",
      "finish year 2010 and month 12\n"
     ]
    }
   ],
   "source": [
    "Economist = E\n",
    "\n",
    "Time_y = []\n",
    "Time_m = []\n",
    "Target_num = []\n",
    "Total_word_num = []\n",
    "Target_percent = []\n",
    "\n",
    "target = 'national'\n",
    "tag = 'Asia'\n",
    "# 'Business', 'Finance and economics', 'Science and technology', 'Books and arts'\n",
    "#  'Britain', 'Europe', 'United States', 'The Americas', 'Middle East and Africa', 'Asia',\n",
    "for year in years: \n",
    "    E_year = Economist[Economist.Year == int(year)]\n",
    "    for month in range(1,13):\n",
    "        E_month = E_year[E_year.Month == month]\n",
    "        E_sector = E_month[E_month.Tag == tag]\n",
    "        E_sector = E_sector.reset_index()\n",
    "        del E_sector['index']\n",
    "        \n",
    "        total_word_num = 0 # monthly\n",
    "        target_num = 0\n",
    "        for i in range(0, len(E_sector)-1):\n",
    "            article = E_sector['Article'][i]\n",
    "            \n",
    "            article_tokens = nltk.word_tokenize(article)\n",
    "            article_countedWords = wordCounter(article_tokens)\n",
    "            total_word_num = total_word_num + sum(list(article_countedWords['count']))\n",
    "\n",
    "        \n",
    "            if list(article_countedWords[article_countedWords.word == target]['count']) == []:\n",
    "                target_num = target_num + 0\n",
    "            else: \n",
    "                target_num = target_num + list(article_countedWords[article_countedWords.word == target]['count'])[0]\n",
    "        \n",
    "        \n",
    "        Target_num.append(target_num)\n",
    "        Time_y.append(int(year))\n",
    "        Time_m.append(month)\n",
    "        Total_word_num.append(total_word_num)\n",
    "        Target_percent.append(target_num/total_word_num)\n",
    "        print('finish year '+ year+' and month '+str(month))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_table_All = pandas.DataFrame({'Time_y': Time_y, 'Time_m': Time_m, \n",
    "                                   target+'_num': Target_num,\n",
    "                                  'Total_word_num':Total_word_num,\n",
    "                                  'Target_percent': Target_percent})\n",
    "Freq_table_All = Freq_table_All[['Time_y', 'Time_m', 'Total_word_num', 'Target_percent',  target+'_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_table_All.to_csv('Freq_table_Asia_crisis.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:purple\"> Week 4: Word Embeddings\n",
    "<span style=\"color:purple\"> Fig 1: visualize the word \"bush\", \"obama\", \"beranke\" by year <br>\n",
    "<span style=\"color:purple\"> Pairs: X+Y = Z+ <br>\n",
    "<span style=\"color:purple\"> Fig 3: Loop the different regions/ areas {'Britain', 'Europe', 'United States', 'The Americas', 'Middle East and Africa', 'Asia'} <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Article</th>\n",
       "      <th>Tokenized_Words</th>\n",
       "      <th>Normalized_Words</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Nuclear proliferation Misreading Iran</td>\n",
       "      <td>Leaders</td>\n",
       "      <td>\\n\\t\\t\\tSpecial report Iran's nuclear programm...</td>\n",
       "      <td>['Special', 'report', 'Iran', \"'s\", 'nuclear',...</td>\n",
       "      <td>['special', 'report', 'iran', 'nuclear', 'prog...</td>\n",
       "      <td>[['Special', 'report', 'Iran', \"'s\", 'nuclear'...</td>\n",
       "      <td>[['special', 'report', 'iran', 'nuclear', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>UN reform Fix it or scrap it</td>\n",
       "      <td>Leaders</td>\n",
       "      <td>\\n\\t\\t\\tMexico and the United States Shots acr...</td>\n",
       "      <td>['Mexico', 'and', 'the', 'United', 'States', '...</td>\n",
       "      <td>['mexico', 'unit', 'state', 'shot', 'across', ...</td>\n",
       "      <td>[['Mexico', 'and', 'the', 'United', 'States', ...</td>\n",
       "      <td>[['mexico', 'united', 'states', 'shots', 'acro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Spain and its regions A Catalan kerfuffle</td>\n",
       "      <td>Leaders</td>\n",
       "      <td>\\n\\t\\t\\tSpain and Catalonia Bad echoes from th...</td>\n",
       "      <td>['Spain', 'and', 'Catalonia', 'Bad', 'echoes',...</td>\n",
       "      <td>['spain', 'catalonia', 'bad', 'echo', 'past', ...</td>\n",
       "      <td>[['Spain', 'and', 'Catalonia', 'Bad', 'echoes'...</td>\n",
       "      <td>[['spain', 'catalonia', 'bad', 'echoes', 'past...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Year  Month  Day                                      Title  \\\n",
       "0           0  2006      1   14      Nuclear proliferation Misreading Iran   \n",
       "1           1  2006      1   14               UN reform Fix it or scrap it   \n",
       "2           2  2006      1   14  Spain and its regions A Catalan kerfuffle   \n",
       "\n",
       "       Tag                                            Article  \\\n",
       "0  Leaders  \\n\\t\\t\\tSpecial report Iran's nuclear programm...   \n",
       "1  Leaders  \\n\\t\\t\\tMexico and the United States Shots acr...   \n",
       "2  Leaders  \\n\\t\\t\\tSpain and Catalonia Bad echoes from th...   \n",
       "\n",
       "                                     Tokenized_Words  \\\n",
       "0  ['Special', 'report', 'Iran', \"'s\", 'nuclear',...   \n",
       "1  ['Mexico', 'and', 'the', 'United', 'States', '...   \n",
       "2  ['Spain', 'and', 'Catalonia', 'Bad', 'echoes',...   \n",
       "\n",
       "                                    Normalized_Words  \\\n",
       "0  ['special', 'report', 'iran', 'nuclear', 'prog...   \n",
       "1  ['mexico', 'unit', 'state', 'shot', 'across', ...   \n",
       "2  ['spain', 'catalonia', 'bad', 'echo', 'past', ...   \n",
       "\n",
       "                                     tokenized_sents  \\\n",
       "0  [['Special', 'report', 'Iran', \"'s\", 'nuclear'...   \n",
       "1  [['Mexico', 'and', 'the', 'United', 'States', ...   \n",
       "2  [['Spain', 'and', 'Catalonia', 'Bad', 'echoes'...   \n",
       "\n",
       "                                    normalized_sents  \n",
       "0  [['special', 'report', 'iran', 'nuclear', 'pro...  \n",
       "1  [['mexico', 'united', 'states', 'shots', 'acro...  \n",
       "2  [['spain', 'catalonia', 'bad', 'echoes', 'past...  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Economist[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Article</th>\n",
       "      <th>Tokenized_Words</th>\n",
       "      <th>Normalized_Words</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Nuclear proliferation Misreading Iran</td>\n",
       "      <td>Leaders</td>\n",
       "      <td>\\n\\t\\t\\tSpecial report Iran's nuclear programm...</td>\n",
       "      <td>['Special', 'report', 'Iran', \"'s\", 'nuclear',...</td>\n",
       "      <td>['special', 'report', 'iran', 'nuclear', 'prog...</td>\n",
       "      <td>[[Special, report, Iran, 's, nuclear, programm...</td>\n",
       "      <td>[[special, report, iran, nuclear, programme, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>UN reform Fix it or scrap it</td>\n",
       "      <td>Leaders</td>\n",
       "      <td>\\n\\t\\t\\tMexico and the United States Shots acr...</td>\n",
       "      <td>['Mexico', 'and', 'the', 'United', 'States', '...</td>\n",
       "      <td>['mexico', 'unit', 'state', 'shot', 'across', ...</td>\n",
       "      <td>[[Mexico, and, the, United, States, Shots, acr...</td>\n",
       "      <td>[[mexico, united, states, shots, across, borde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Spain and its regions A Catalan kerfuffle</td>\n",
       "      <td>Leaders</td>\n",
       "      <td>\\n\\t\\t\\tSpain and Catalonia Bad echoes from th...</td>\n",
       "      <td>['Spain', 'and', 'Catalonia', 'Bad', 'echoes',...</td>\n",
       "      <td>['spain', 'catalonia', 'bad', 'echo', 'past', ...</td>\n",
       "      <td>[[Spain, and, Catalonia, Bad, echoes, from, th...</td>\n",
       "      <td>[[spain, catalonia, bad, echoes, past, madrid,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Walls and fences On America's southern border</td>\n",
       "      <td>Leaders</td>\n",
       "      <td>n.a.</td>\n",
       "      <td>['n.a', '.']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[n.a, .]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Anti-social behaviour Soothing the savage breast</td>\n",
       "      <td>Britain</td>\n",
       "      <td>\\n\\t\\t\\tBritain\\n\\t\\t\\tAnti-social behaviour S...</td>\n",
       "      <td>['Britain', 'Anti-social', 'behaviour', 'Sooth...</td>\n",
       "      <td>['britain', 'behaviour', 'sooth', 'savag', 'br...</td>\n",
       "      <td>[[Britain, Anti-social, behaviour, Soothing, t...</td>\n",
       "      <td>[[britain, behaviour, soothing, savage, breast...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  Day                                             Title  \\\n",
       "0  2006      1   14             Nuclear proliferation Misreading Iran   \n",
       "1  2006      1   14                      UN reform Fix it or scrap it   \n",
       "2  2006      1   14         Spain and its regions A Catalan kerfuffle   \n",
       "3  2006      1   14     Walls and fences On America's southern border   \n",
       "4  2006      1   14  Anti-social behaviour Soothing the savage breast   \n",
       "\n",
       "       Tag                                            Article  \\\n",
       "0  Leaders  \\n\\t\\t\\tSpecial report Iran's nuclear programm...   \n",
       "1  Leaders  \\n\\t\\t\\tMexico and the United States Shots acr...   \n",
       "2  Leaders  \\n\\t\\t\\tSpain and Catalonia Bad echoes from th...   \n",
       "3  Leaders                                               n.a.   \n",
       "4  Britain  \\n\\t\\t\\tBritain\\n\\t\\t\\tAnti-social behaviour S...   \n",
       "\n",
       "                                     Tokenized_Words  \\\n",
       "0  ['Special', 'report', 'Iran', \"'s\", 'nuclear',...   \n",
       "1  ['Mexico', 'and', 'the', 'United', 'States', '...   \n",
       "2  ['Spain', 'and', 'Catalonia', 'Bad', 'echoes',...   \n",
       "3                                       ['n.a', '.']   \n",
       "4  ['Britain', 'Anti-social', 'behaviour', 'Sooth...   \n",
       "\n",
       "                                    Normalized_Words  \\\n",
       "0  ['special', 'report', 'iran', 'nuclear', 'prog...   \n",
       "1  ['mexico', 'unit', 'state', 'shot', 'across', ...   \n",
       "2  ['spain', 'catalonia', 'bad', 'echo', 'past', ...   \n",
       "3                                                 []   \n",
       "4  ['britain', 'behaviour', 'sooth', 'savag', 'br...   \n",
       "\n",
       "                                     tokenized_sents  \\\n",
       "0  [[Special, report, Iran, 's, nuclear, programm...   \n",
       "1  [[Mexico, and, the, United, States, Shots, acr...   \n",
       "2  [[Spain, and, Catalonia, Bad, echoes, from, th...   \n",
       "3                                         [[n.a, .]]   \n",
       "4  [[Britain, Anti-social, behaviour, Soothing, t...   \n",
       "\n",
       "                                    normalized_sents  \n",
       "0  [[special, report, iran, nuclear, programme, s...  \n",
       "1  [[mexico, united, states, shots, across, borde...  \n",
       "2  [[spain, catalonia, bad, echoes, past, madrid,...  \n",
       "3                                               [[]]  \n",
       "4  [[britain, behaviour, soothing, savage, breast...  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Economist['tokenized_sents'] = Economist['Article'].apply(lambda x: \n",
    "                                                               [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "Economist['normalized_sents'] = Economist['tokenized_sents'].apply(lambda x: \n",
    "                                                                           [lucem_illud.normalizeTokens(s, \n",
    "                                                                                           stopwordLst = lucem_illud.stop_words_basic, \n",
    "                                                                                           stemmer = None) \n",
    "                                                                            for s in x])\n",
    "\n",
    "Economist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "Economist.to_csv('Economist_15th.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Economist = pandas.read_csv('Economist_15th.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Article</th>\n",
       "      <th>Tokenized_Words</th>\n",
       "      <th>Normalized_Words</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Nuclear proliferation Misreading Iran</td>\n",
       "      <td>Leaders</td>\n",
       "      <td>\\n\\t\\t\\tSpecial report Iran's nuclear programm...</td>\n",
       "      <td>['Special', 'report', 'Iran', \"'s\", 'nuclear',...</td>\n",
       "      <td>['special', 'report', 'iran', 'nuclear', 'prog...</td>\n",
       "      <td>[['Special', 'report', 'Iran', \"'s\", 'nuclear'...</td>\n",
       "      <td>[['special', 'report', 'iran', 'nuclear', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>UN reform Fix it or scrap it</td>\n",
       "      <td>Leaders</td>\n",
       "      <td>\\n\\t\\t\\tMexico and the United States Shots acr...</td>\n",
       "      <td>['Mexico', 'and', 'the', 'United', 'States', '...</td>\n",
       "      <td>['mexico', 'unit', 'state', 'shot', 'across', ...</td>\n",
       "      <td>[['Mexico', 'and', 'the', 'United', 'States', ...</td>\n",
       "      <td>[['mexico', 'united', 'states', 'shots', 'acro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Spain and its regions A Catalan kerfuffle</td>\n",
       "      <td>Leaders</td>\n",
       "      <td>\\n\\t\\t\\tSpain and Catalonia Bad echoes from th...</td>\n",
       "      <td>['Spain', 'and', 'Catalonia', 'Bad', 'echoes',...</td>\n",
       "      <td>['spain', 'catalonia', 'bad', 'echo', 'past', ...</td>\n",
       "      <td>[['Spain', 'and', 'Catalonia', 'Bad', 'echoes'...</td>\n",
       "      <td>[['spain', 'catalonia', 'bad', 'echoes', 'past...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  Day                                      Title      Tag  \\\n",
       "0  2006      1   14      Nuclear proliferation Misreading Iran  Leaders   \n",
       "1  2006      1   14               UN reform Fix it or scrap it  Leaders   \n",
       "2  2006      1   14  Spain and its regions A Catalan kerfuffle  Leaders   \n",
       "\n",
       "                                             Article  \\\n",
       "0  \\n\\t\\t\\tSpecial report Iran's nuclear programm...   \n",
       "1  \\n\\t\\t\\tMexico and the United States Shots acr...   \n",
       "2  \\n\\t\\t\\tSpain and Catalonia Bad echoes from th...   \n",
       "\n",
       "                                     Tokenized_Words  \\\n",
       "0  ['Special', 'report', 'Iran', \"'s\", 'nuclear',...   \n",
       "1  ['Mexico', 'and', 'the', 'United', 'States', '...   \n",
       "2  ['Spain', 'and', 'Catalonia', 'Bad', 'echoes',...   \n",
       "\n",
       "                                    Normalized_Words  \\\n",
       "0  ['special', 'report', 'iran', 'nuclear', 'prog...   \n",
       "1  ['mexico', 'unit', 'state', 'shot', 'across', ...   \n",
       "2  ['spain', 'catalonia', 'bad', 'echo', 'past', ...   \n",
       "\n",
       "                                     tokenized_sents  \\\n",
       "0  [['Special', 'report', 'Iran', \"'s\", 'nuclear'...   \n",
       "1  [['Mexico', 'and', 'the', 'United', 'States', ...   \n",
       "2  [['Spain', 'and', 'Catalonia', 'Bad', 'echoes'...   \n",
       "\n",
       "                                    normalized_sents  \n",
       "0  [['special', 'report', 'iran', 'nuclear', 'pro...  \n",
       "1  [['mexico', 'united', 'states', 'shots', 'acro...  \n",
       "2  [['spain', 'catalonia', 'bad', 'echoes', 'past...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del Economist['Unnamed: 0']\n",
    "Economist[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:purple\"> Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics = ['Leaders', 'Britain', 'Europe', 'United States', 'The Americas', \n",
    "#'Middle East and Africa', 'Asia', 'Obituary',\n",
    "#'Business', 'Finance and economics', 'Science and technology', 'Books and arts']\n",
    "\n",
    "E = pandas.read_csv('Economist_15th.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "Economist = E\n",
    "E_year = Economist[Economist.Year == 2009]\n",
    "E_area = E_year[E_year.Tag == 'Science and technology']\n",
    "Economist = E_area\n",
    "#Economist = E_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Article</th>\n",
       "      <th>Tokenized_Words</th>\n",
       "      <th>Normalized_Words</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10554</th>\n",
       "      <td>10554</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Numbers Easy as 1, 2,</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...</td>\n",
       "      <td>['Plenty', 'more', 'fish', 'in', 'the', 'sea',...</td>\n",
       "      <td>['plenti', 'fish', 'sea', 'longer', 'technolog...</td>\n",
       "      <td>[['Plenty', 'more', 'fish', 'in', 'the', 'sea'...</td>\n",
       "      <td>[['plenty', 'fish', 'sea'], ['longer', 'techno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555</th>\n",
       "      <td>10555</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>64 More numbers When 1,2, 3... is not enough</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...</td>\n",
       "      <td>['Plenty', 'more', 'fish', 'in', 'the', 'sea',...</td>\n",
       "      <td>['plenti', 'fish', 'sea', 'longer', 'technolog...</td>\n",
       "      <td>[['Plenty', 'more', 'fish', 'in', 'the', 'sea'...</td>\n",
       "      <td>[['plenty', 'fish', 'sea'], ['longer', 'techno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Year  Month  Day  \\\n",
       "10554       10554  2009      1    3   \n",
       "10555       10555  2009      1    3   \n",
       "\n",
       "                                              Title                     Tag  \\\n",
       "10554                         Numbers Easy as 1, 2,  Science and technology   \n",
       "10555  64 More numbers When 1,2, 3... is not enough  Science and technology   \n",
       "\n",
       "                                                 Article  \\\n",
       "10554  \\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...   \n",
       "10555  \\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...   \n",
       "\n",
       "                                         Tokenized_Words  \\\n",
       "10554  ['Plenty', 'more', 'fish', 'in', 'the', 'sea',...   \n",
       "10555  ['Plenty', 'more', 'fish', 'in', 'the', 'sea',...   \n",
       "\n",
       "                                        Normalized_Words  \\\n",
       "10554  ['plenti', 'fish', 'sea', 'longer', 'technolog...   \n",
       "10555  ['plenti', 'fish', 'sea', 'longer', 'technolog...   \n",
       "\n",
       "                                         tokenized_sents  \\\n",
       "10554  [['Plenty', 'more', 'fish', 'in', 'the', 'sea'...   \n",
       "10555  [['Plenty', 'more', 'fish', 'in', 'the', 'sea'...   \n",
       "\n",
       "                                        normalized_sents  \n",
       "10554  [['plenty', 'fish', 'sea'], ['longer', 'techno...  \n",
       "10555  [['plenty', 'fish', 'sea'], ['longer', 'techno...  "
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Economist[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Economist['Tokenized_Words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Economist['Normalized_Words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Economist['tokenized_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Economist['normalized_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Economist['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10554</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Numbers Easy as 1, 2,</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>64 More numbers When 1,2, 3... is not enough</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  Month  Day                                         Title  \\\n",
       "10554  2009      1    3                         Numbers Easy as 1, 2,   \n",
       "10555  2009      1    3  64 More numbers When 1,2, 3... is not enough   \n",
       "\n",
       "                          Tag  \\\n",
       "10554  Science and technology   \n",
       "10555  Science and technology   \n",
       "\n",
       "                                                 Article  \n",
       "10554  \\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...  \n",
       "10555  \\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...  "
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Economist[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Article</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10554</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Numbers Easy as 1, 2,</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...</td>\n",
       "      <td>[[Plenty, more, fish, in, the, sea, ?], [No, l...</td>\n",
       "      <td>[[plenty, fish, sea], [longer, technology, mad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>64 More numbers When 1,2, 3... is not enough</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...</td>\n",
       "      <td>[[Plenty, more, fish, in, the, sea, ?], [No, l...</td>\n",
       "      <td>[[plenty, fish, sea], [longer, technology, mad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10635</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>Science policy Blessed are the geeks, for they...</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tScience and technology\\n\\t\\t\\tBlessed ...</td>\n",
       "      <td>[[Science, and, technology, Blessed, are, the,...</td>\n",
       "      <td>[[science, technology, blessed, geeks, shall, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10636</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>The environment Green Bush</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tAppointments DFD gainst world poverty ...</td>\n",
       "      <td>[[Appointments, DFD, gainst, world, poverty, S...</td>\n",
       "      <td>[[appointments, dfd, gainst, world, poverty, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10637</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>Astronomy The cosmic boogie-box</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tThe cosmic boogie-box\\n\\t\\t\\tAstronomy...</td>\n",
       "      <td>[[The, cosmic, boogie-box, Astronomy, The, sky...</td>\n",
       "      <td>[[cosmic, astronomy, sky, seems, filled, unexp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  Month  Day                                              Title  \\\n",
       "10554  2009      1    3                              Numbers Easy as 1, 2,   \n",
       "10555  2009      1    3       64 More numbers When 1,2, 3... is not enough   \n",
       "10635  2009      1   10  Science policy Blessed are the geeks, for they...   \n",
       "10636  2009      1   10                         The environment Green Bush   \n",
       "10637  2009      1   10                    Astronomy The cosmic boogie-box   \n",
       "\n",
       "                          Tag  \\\n",
       "10554  Science and technology   \n",
       "10555  Science and technology   \n",
       "10635  Science and technology   \n",
       "10636  Science and technology   \n",
       "10637  Science and technology   \n",
       "\n",
       "                                                 Article  \\\n",
       "10554  \\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...   \n",
       "10555  \\n\\t\\t\\tPlenty more fish in the sea?\\n\\t\\t\\tNo...   \n",
       "10635  \\n\\t\\t\\tScience and technology\\n\\t\\t\\tBlessed ...   \n",
       "10636  \\n\\t\\t\\tAppointments DFD gainst world poverty ...   \n",
       "10637  \\n\\t\\t\\tThe cosmic boogie-box\\n\\t\\t\\tAstronomy...   \n",
       "\n",
       "                                         tokenized_sents  \\\n",
       "10554  [[Plenty, more, fish, in, the, sea, ?], [No, l...   \n",
       "10555  [[Plenty, more, fish, in, the, sea, ?], [No, l...   \n",
       "10635  [[Science, and, technology, Blessed, are, the,...   \n",
       "10636  [[Appointments, DFD, gainst, world, poverty, S...   \n",
       "10637  [[The, cosmic, boogie-box, Astronomy, The, sky...   \n",
       "\n",
       "                                        normalized_sents  \n",
       "10554  [[plenty, fish, sea], [longer, technology, mad...  \n",
       "10555  [[plenty, fish, sea], [longer, technology, mad...  \n",
       "10635  [[science, technology, blessed, geeks, shall, ...  \n",
       "10636  [[appointments, dfd, gainst, world, poverty, s...  \n",
       "10637  [[cosmic, astronomy, sky, seems, filled, unexp...  "
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and normalize the texts\n",
    "Economist['tokenized_sents'] = Economist['Article'].apply(lambda x: \n",
    "                                                               [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "Economist['normalized_sents'] = Economist['tokenized_sents'].apply(lambda x: \n",
    "                                                                           [lucem_illud.normalizeTokens(s, \n",
    "                                                                                           stopwordLst = lucem_illud.stop_words_basic, \n",
    "                                                                                           stemmer = None) \n",
    "                                                                            for s in x])\n",
    "\n",
    "Economist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word to vector\n",
    "EconomistW2V = gensim.models.word2vec.Word2Vec(Economist['normalized_sents'].sum())\n",
    "# see the vector of word \"fish\"\n",
    "# CookBookW2V['fish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# words and their frequencies -- generate the list of representative words\n",
    "Economist['tokenized_text'] = Economist['Article'].apply(lambda x: nltk.word_tokenize(x))\n",
    "countsDict = {}\n",
    "for word in Economist['tokenized_text'].sum():\n",
    "    if word in countsDict:\n",
    "        countsDict[word] += 1\n",
    "    else:\n",
    "        countsDict[word] = 1\n",
    "word_counts = sorted(countsDict.items(), key = lambda x : x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 0.9997553825378418),\n",
       " ('energy', 0.9997526407241821),\n",
       " ('tion', 0.9997520446777344),\n",
       " ('research', 0.9997515678405762),\n",
       " ('even', 0.9997509121894836),\n",
       " ('also', 0.9997484087944031),\n",
       " ('less', 0.9997453689575195),\n",
       " ('high', 0.9997453689575195),\n",
       " ('new', 0.999744713306427),\n",
       " ('fish', 0.999742865562439)]"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EconomistW2V.most_similar('crisis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Article</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12097</th>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>Wolfram Alpha The search is on</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tThe revolution within\\n\\t\\t\\tTheway ba...</td>\n",
       "      <td>[[The, revolution, within, Theway, banks, mana...</td>\n",
       "      <td>[[revolution, within, theway, banks, manage, h...</td>\n",
       "      <td>[The, revolution, within, Theway, banks, manag...</td>\n",
       "      <td>[revolution, within, theway, banks, manage, ho...</td>\n",
       "      <td>1806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13695</th>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>Paying to save trees Last gasp for the forest</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tSaence and technology\\n\\t\\t\\tLast gasp...</td>\n",
       "      <td>[[Saence, and, technology, Last, gasp, for, th...</td>\n",
       "      <td>[[saence, technology, last, gasp, forest, payi...</td>\n",
       "      <td>[Saence, and, technology, Last, gasp, for, the...</td>\n",
       "      <td>[saence, technology, last, gasp, forest, payin...</td>\n",
       "      <td>1576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14483</th>\n",
       "      <td>2009</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>Climate change What lies beneath</td>\n",
       "      <td>Science and technology</td>\n",
       "      <td>\\n\\t\\t\\tAdvertising feature 1 Towards a sustai...</td>\n",
       "      <td>[[Advertising, feature, 1, Towards, a, sustain...</td>\n",
       "      <td>[[advertising, feature, towards, sustainable, ...</td>\n",
       "      <td>[Advertising, feature, 1, Towards, a, sustaina...</td>\n",
       "      <td>[advertising, feature, towards, sustainable, f...</td>\n",
       "      <td>1456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  Month  Day                                          Title  \\\n",
       "12097  2009      5   16                 Wolfram Alpha The search is on   \n",
       "13695  2009      9   26  Paying to save trees Last gasp for the forest   \n",
       "14483  2009     12    5               Climate change What lies beneath   \n",
       "\n",
       "                          Tag  \\\n",
       "12097  Science and technology   \n",
       "13695  Science and technology   \n",
       "14483  Science and technology   \n",
       "\n",
       "                                                 Article  \\\n",
       "12097  \\n\\t\\t\\tThe revolution within\\n\\t\\t\\tTheway ba...   \n",
       "13695  \\n\\t\\t\\tSaence and technology\\n\\t\\t\\tLast gasp...   \n",
       "14483  \\n\\t\\t\\tAdvertising feature 1 Towards a sustai...   \n",
       "\n",
       "                                         tokenized_sents  \\\n",
       "12097  [[The, revolution, within, Theway, banks, mana...   \n",
       "13695  [[Saence, and, technology, Last, gasp, for, th...   \n",
       "14483  [[Advertising, feature, 1, Towards, a, sustain...   \n",
       "\n",
       "                                        normalized_sents  \\\n",
       "12097  [[revolution, within, theway, banks, manage, h...   \n",
       "13695  [[saence, technology, last, gasp, forest, payi...   \n",
       "14483  [[advertising, feature, towards, sustainable, ...   \n",
       "\n",
       "                                          tokenized_text  \\\n",
       "12097  [The, revolution, within, Theway, banks, manag...   \n",
       "13695  [Saence, and, technology, Last, gasp, for, the...   \n",
       "14483  [Advertising, feature, 1, Towards, a, sustaina...   \n",
       "\n",
       "                                       normalized_tokens  \\\n",
       "12097  [revolution, within, theway, banks, manage, ho...   \n",
       "13695  [saence, technology, last, gasp, forest, payin...   \n",
       "14483  [advertising, feature, towards, sustainable, f...   \n",
       "\n",
       "       normalized_tokens_count  \n",
       "12097                     1806  \n",
       "13695                     1576  \n",
       "14483                     1456  "
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will only target the top 200 words in the list\n",
    "numWords = 100\n",
    "targetWords = EconomistW2V.wv.index2word[150:numWords]\n",
    "\n",
    "\n",
    "stop_words_nltk = stopwords.words('english')\n",
    "\n",
    "#The stemmers and lemmers need to be initialized before bing run\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english')\n",
    "wordnet = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def normlizeTokens(tokenLst, stopwordLst = None, stemmer = None, lemmer = None):\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "        \n",
    "    #And the lemmer\n",
    "    if lemmer is not None:\n",
    "        workingIter = (lemmer.lemmatize(w) for w in workingIter)\n",
    "    \n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "    #We will return a list with the stopwords removed\n",
    "    return list(workingIter)\n",
    "\n",
    "Economist['normalized_tokens'] = Economist['tokenized_text'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = None))\n",
    "\n",
    "Economist['normalized_tokens_count'] = Economist['normalized_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "Economist.sort_values(by='normalized_tokens_count', ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsSubMatrix = []\n",
    "for word in targetWords:\n",
    "    wordsSubMatrix.append(EconomistW2V[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWords = sklearn.manifold.TSNE(n_components = 2, early_exaggeration = 25).fit_transform(reducedPCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "for i, word in enumerate(targetWords):\n",
    "    ax.annotate(word, \n",
    "                (tsneWords[:, 0][i],tsneWords[:, 1][i]), \n",
    "                size =  20 * (numWords - i) / numWords, \n",
    "                alpha = .8 * (numWords - i) / numWords + .2)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:purple\"> Week 8 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topics = ['Leaders', 'Britain', 'Europe', 'United States', 'The Americas', \n",
    "              'Middle East and Africa', 'Asia', 'Obituary',\n",
    "              'Business', 'Finance and economics', 'Science and technology', 'Books and arts']\n",
    "\n",
    "years = ['2006','2007','2008','2009','2010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nodes = []\n",
    "Title = []\n",
    "for t in Topics: \n",
    "    f = open('ALL' + t +'.txt','r')\n",
    "    file = f.read()\n",
    "    f.close()\n",
    "    Nodes.append(file)\n",
    "    Title.append('ALL' + t +'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = pandas.DataFrame({'Nodes': Nodes, 'Title': Title})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw['normalized_sentences'] = nw['tokenized_sentences'].apply(lambda x: [lucem_illud.normalizeTokens(s, stopwordLst = lucem_illud.stop_words_basic, stemmer = lucem_illud.stemmer_basic) for s in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCooccurrence(sentences, makeMatrix = False):\n",
    "    words = set()\n",
    "    for sent in sentences:\n",
    "        words |= set(sent)\n",
    "    wordLst = list(words)\n",
    "    wordIndices = {w: i for i, w in enumerate(wordLst)}\n",
    "    wordCoCounts = {}\n",
    "    #consider a sparse matrix if memory becomes an issue\n",
    "    coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\n",
    "    for sent in sentences:\n",
    "        for i, word1 in enumerate(sent):\n",
    "            word1Index = wordIndices[word1]\n",
    "            for word2 in sent[i + 1:]:\n",
    "                coOcMat[word1Index][wordIndices[word2]] += 1\n",
    "    if makeMatrix:\n",
    "        return coOcMat, wordLst\n",
    "    else:\n",
    "        coOcMat = coOcMat.T + coOcMat\n",
    "        g = nx.convert_matrix.from_numpy_matrix(coOcMat)\n",
    "        g = nx.relabel_nodes(g, {i : w for i, w in enumerate(wordLst)})\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nw = wordCooccurrence(nw['normalized_sentences'][:100].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(g_nw)\n",
    "plt.savefig(\"nw.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:purple\"> Document-Document Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenlist = nltk.word_tokenize(text)\n",
    "    normalized = lucem_illud.normalizeTokens(tokenlist, stopwordLst = lucem_illud.stop_words_basic, stemmer = lucem_illud.stemmer_basic)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "senVectorizer = sklearn.feature_extraction.text.CountVectorizer(tokenizer = tokenize)\n",
    "senVects_incidence_nw = senVectorizer.fit_transform(nw['Nodes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "g_2mode = nx.Graph()\n",
    "\n",
    "#define all the nodes\n",
    "g_2mode.add_nodes_from((senVectorizer.get_feature_names()[i] for i in range(senVects_incidence_nw.shape[1])), bipartite = 'word')\n",
    "g_2mode.add_nodes_from(range(senVects_incidence_nw.shape[0]), bipartite = 'doc')\n",
    "#add all the edges\n",
    "g_2mode.add_edges_from(((d, senVectorizer.get_feature_names()[w], {'weight' : senVects_incidence_nw[d, w]}) for d, w in zip(*senVects_incidence_nw.nonzero())))\n",
    "\n",
    "print(nx.info(g_2mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractNetwork(g, targetType):\n",
    "    g_mono = nx.Graph()\n",
    "    g_mono.add_nodes_from(((n, d) for n, d in g.nodes(data = True) if d['bipartite'] == targetType))\n",
    "    \n",
    "    for n_outside in (n for n, d in g.nodes(data = True) if d['bipartite'] != targetType):\n",
    "        neighbors = list((n for n in g.neighbors(n_outside) if g.nodes[n]['bipartite'] == targetType))\n",
    "        for i, n1 in enumerate(neighbors):\n",
    "            for n2 in neighbors[i+1:]:\n",
    "                try:\n",
    "                    g_mono.edges[n1, n2]['weight'] += 1\n",
    "                except KeyError:\n",
    "                    g_mono.add_edge(n1, n2, weight = 1)\n",
    "    return g_mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 0\n",
      "Number of edges: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gDoc = contractNetwork(g_2mode, 'doc')\n",
    "# Document to document network\n",
    "print(nx.info(gDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd0AAAFDCAYAAAB/UdRdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAABQNJREFUeJzt1TEBwCAQwMBS/54fFYSBOwXZsmZmPgDguP92AAC8wnQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASBiugAQMV0AiJguAERMFwAipgsAEdMFgIjpAkDEdAEgYroAEDFdAIiYLgBETBcAIqYLABHTBYCI6QJAxHQBIGK6ABAxXQCImC4AREwXACKmCwAR0wWAiOkCQMR0ASCyAR3iBoLgmtTtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layout = nx.spring_layout(gDoc, k = 1/3, weight='weight', iterations= 50)\n",
    "nx.draw(gDoc, pos = layout, labels = {n:n for n in gDoc.nodes()}) #Getting labels is a bit annoying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
